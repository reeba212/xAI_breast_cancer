# xAI_breast_cancer

Hello everyone! 
Welcome to my repository.

Through this project, I wanted to build a model that could contribute to the healthcare field using Artificial Intelligence. However, as health is a crucial and delicate matter, healthcare professionals hesitate in using AI to make diagnosis, as AI acts as a black box, providing no explanation for it's predictions. A recent development solves this issue, called Explanable AI (xAI). It is an artificial intelligence in which humans can understand the decisions or predictions made by the AI. It contrasts with the "black box" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.

I selected a dataset of Breast Histology Pictures from Kaggle. This dataset, which was collected from Andrew Janowczyk's website and utilized for a data science class at Epidemium, comprises 5547 breast histology photos with a size of 50 × 50 x 3. Classifying malignant pictures (IDC: invasive ductal carcinoma) vs non-IDC images is the objective. 

A CNN is a particular type of network design for deep learning algorithms that is utilized for tasks like image recognition and pixel data processing. Although there are different kinds of neural networks in deep learning, CNNs are the preferred network architecture for identifying and recognising objects. 

In this project, I made use of the pretrained ResNet-50 model, a convolutional neural network with 50 layers, pre-trained on more than a million photos from the ImageNet collection. This method of using a pre-trained model to solve a new problem is called Transfer Learning. Transfer Learning's key benefits are reduced training time, improved neural network performance, and a lack of big dataset requirements. Using ResNet-50, we also added several dense and flattening layers to it using Keras, the details of which can be seen in Figure 1. The images were resized to (224,224) pixels as it’s established that this size works best on ResNet-50. I used RMSprop optimizer and categorical cross entropy loss function to train the model. It achieved training accuracy of  93.8021% and validation accuracy of 81.4414%.

There are two types of Explainable AI methods: those that directly communicate the model's inner workings (so-called transparency) or those that explain how the model came to its predictions (so-called post-hoc explanation). In this approach, post-hoc justification is used. A trained and/or tested AI model is used as input by a post-hoc XAI approach, which then creates intelligible representations in the form of feature significance scores, rule sets, heatmaps, or natural language to approximate the inner workings and decision logic of the model.

I used the LIME (Local Interpretable Model-Agnostic Explanations) method. By varying the input of data samples and observing how the predictions change, the LIME technique seeks to comprehend the model. To explain each particular prediction, it approximates any black box machine learning model with a local, understandable model.

I used the scikit-learn mark boundaries method to visualize the explanation after obtaining it from LIME. The final image demonstrates which elements of the original image form the basis for the model's forecast. The image will have hues from green to red. Green pixels reflect the portions of the image that the model used to make its prediction, while red pixels represent the portions of the image that did not support it. Another great method is heat map visualization which performs the same actions on a scale of blue to red. Both of these methods highlight those areas of the images which support the prediction made by the model and hence increase the explainability, which was our goal.
